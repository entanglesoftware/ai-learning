{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qh3Dj6XLBWEC",
        "outputId": "2dc66916-f235-4b14-be88-36a24af64604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: sympy 1.13.3\n",
            "Uninstalling sympy-1.13.3:\n",
            "  Successfully uninstalled sympy-1.13.3\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "Installing collected packages: sympy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "547395ac77214f93a1815e2b92435d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classify the given temperature as Cold, Warm, or Hot:\n",
            "\n",
            "\n",
            "Temperature: 0°C\n",
            "Condition: Cold\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 10°C\n",
            "Condition: Cold\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 20°C\n",
            "Condition: Warm\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 30°C\n",
            "Condition: Hot\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 40°C\n",
            "Condition: Hot\n",
            "\n",
            "\n",
            "Temperature: 25°C\n",
            "Condition: Hot\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers langchain huggingface_hub langchain_community --quiet\n",
        "!pip uninstall sympy -y\n",
        "!pip install sympy\n",
        "\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, GenerationConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login to Hugging Face\n",
        "HF_TOKEN = \"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\"\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "examples = [\n",
        "    {\"temperature\": \"0°C\", \"condition\": \"Cold\"},\n",
        "    {\"temperature\": \"10°C\", \"condition\": \"Cold\"},\n",
        "    {\"temperature\": \"20°C\", \"condition\": \"Warm\"},\n",
        "    {\"temperature\": \"30°C\", \"condition\": \"Hot\"},\n",
        "    {\"temperature\": \"40°C\", \"condition\": \"Hot\"},\n",
        "]\n",
        "example_template = \"\"\"\n",
        "Temperature: {temperature}\n",
        "Condition: {condition}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"temperature\", \"condition\"],\n",
        "    template=example_template,\n",
        ")\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50,  # Adjust to control how many examples are selected\n",
        ")\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Classify the given temperature as Cold, Warm, or Hot:\",\n",
        "    suffix=\"Temperature: {input}\\nCondition:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        "\n",
        ")\n",
        "# Wrap in LangChain\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"distilgpt2\",\n",
        "    device=0,\n",
        "    generation_config=GenerationConfig(\n",
        "        max_length=50,\n",
        "        max_new_tokens=10,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=50256,  # Stop at EOS\n",
        "    )\n",
        ")\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=generator,\n",
        "    model_kwargs={\"max_length\": 50, \"max_new_tokens\": 10}\n",
        ")\n",
        "\n",
        "\n",
        "query = \"25°C\"\n",
        "prompt_text = dynamic_prompt.format(input=query)\n",
        "\n",
        "# Generate output\n",
        "response = llm(prompt_text)\n",
        "print(response)\n"
      ]
    }
  ]
}